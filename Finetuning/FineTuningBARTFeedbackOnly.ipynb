{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUdW01qDmSju"
      },
      "source": [
        "## Installation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jOwL7dLAKFFm",
        "outputId": "d451047e-0173-4a4c-b9ef-72826bcb561f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.5.0)\n",
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (0.19.9)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: rouge_score in /usr/local/lib/python3.11/dist-packages (0.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.5.2)\n",
            "Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb) (8.1.8)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (3.1.44)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb) (4.3.7)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (5.29.4)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.11.2)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb) (2.25.1)\n",
            "Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb) (1.3.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb) (75.2.0)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4 in /usr/local/lib/python3.11/dist-packages (from wandb) (4.13.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.3.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3->wandb) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "!pip install transformers datasets peft wandb evaluate rouge_score nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ON9rRG1miKm"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqqBaSkkmh2s",
        "outputId": "4655b4d2-7bf4-4b3b-90a6-5653561d8af5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    DataCollatorForSeq2Seq,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from peft import (\n",
        "    get_peft_model,\n",
        "    LoraConfig,\n",
        "    TaskType,\n",
        "    PeftModel,\n",
        "    PeftConfig\n",
        ")\n",
        "import wandb\n",
        "import evaluate\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZcJ04NyB5HR5",
        "outputId": "e3e64029-0dc6-4673-e324-51bcdd2d0851"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tYqYi6bQmpwM"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaLyqViemdma",
        "outputId": "7994647d-71bc-40b5-d296-26f27242ffb5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'id': '6a31b925382d4e31a417cc78399dbff2', 'question': 'What is \"frame bursting\"? Also, give 1 advantage and disadvantage compared to the carrier extension.', 'reference_answer': 'Frame bursting reduces the overhead for transmitting small frames by concatenating a sequence of multiple frames in one single transmission, without ever releasing control of the channel.\\nAdvantage :it is more efficient than carrier extension as single frames not filled up with garbage.\\nDisadvantage :need frames waiting for transmission or buffering and delay of frames', 'provided_answer': 'Frame bursting is a feature for the IEEE 802.3z standard.\\nAdvantage: better efficiency\\nDisadvantage: station has to wait for enough data to send so frames need to wait (n-to-n delay)', 'answer_feedback': 'The response correctly answers the advantage and disadvantage part of the question. However, the definition is missing in the answer. The correct definition is that frame bursting is used to concatenate a sequence of multiple frames and transmitting them in one single transmission to reduce the overhead of transmitting small frames.', 'verification_feedback': 'Partially correct', 'score': 0.5}\n",
            "Train set size: 1700\n",
            "Validation set size: 427\n",
            "Test set size: 375\n"
          ]
        }
      ],
      "source": [
        "# Load the dataset\n",
        "dataset = load_dataset(\"Short-Answer-Feedback/saf_communication_networks_english\")\n",
        "\n",
        "# Examine a sample\n",
        "print(dataset['train'][0])\n",
        "\n",
        "# Get basic statistics\n",
        "print(f\"Train set size: {len(dataset['train'])}\")\n",
        "print(f\"Validation set size: {len(dataset['validation'])}\")\n",
        "print(f\"Test set size: {len(dataset['test_unseen_answers'])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nusVuy3XnDpi"
      },
      "source": [
        "### Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169,
          "referenced_widgets": [
            "c4419bf3847d4124b6288550993b522f",
            "e19ae6f525544696955b7a138353a93d",
            "bcb78d5846f242a681a978e913ebdffd",
            "be2b4ae4d52b4a728e633f8e1594f71a",
            "4ac2c42b25c44b54aa306ee0de04d865",
            "c27657a006fa48e8ba8b495fc94504cc",
            "779e4dce210144c5aec52c5fc84de545",
            "cb4b6104cd0e4b7b9f91d37562983c5b",
            "d0dd3e1576404539828538a0fde58b9e",
            "4f46f9ae2b264e16bf9f2bf9ca747100",
            "196f89f9b3314d698cbc7abec47c086c",
            "abe7c0fb5b2f4befbe55f04726250389",
            "aaa0fae623de4593bc5655b28eba9c7c",
            "62836dfebe7e4415b4f7b636e79412ce",
            "21ff3826331842cca461e4f648256a23",
            "b41da0516290471f8495dd9210cfcc69",
            "74c96253df2446b09fcab538dc9d5273",
            "8073bef2ee4e4e8686a4fc1cc95df03c",
            "3c1ef3d0307e470a97e05821539990e6",
            "a9ae343b1fa5431981c399034b9df5a8",
            "7084c805624f445ba958c3cea8b2b190",
            "bf4379cb253a4849bfec408889802c52",
            "e27f10ce9d604fc6988b745cb24d1ec6",
            "8f58224d29634d5b8069af78f5d955a9",
            "59398cd4d0e5441bbb95e4578e233a14",
            "c9192f0e65d14445bfb0b7674acc48ce",
            "fa37e68a76ff4a9e8d7ddc21bd5fa3ef",
            "07b34f117e2b44b9b06cb3aa36ddf7e8",
            "b5a9f66e21e94618860345f15254c597",
            "367f1323cba945f4848b4757c3814686",
            "aa37c6bda73048389a33b491645a43a5",
            "5e62561040314d588c48584120e90354",
            "d11c529dd3294723b6779172d4f17c69"
          ]
        },
        "id": "nfyPvloqulpt",
        "outputId": "c3c54d58-52a4-4c99-ba47-34ba524b9c6e"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c4419bf3847d4124b6288550993b522f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/1700 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py:3980: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "abe7c0fb5b2f4befbe55f04726250389",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/427 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e27f10ce9d604fc6988b745cb24d1ec6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/375 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Define the preprocessing function\n",
        "def preprocess_function(examples):\n",
        "    # Create inputs that include question, provided answer, and score\n",
        "    inputs = [\n",
        "        f\"Question: {q}\\nStudent Answer: {a}\\nScore: {s}\"\n",
        "        for q, a, s in zip(examples[\"question\"], examples[\"provided_answer\"], examples[\"score\"])\n",
        "    ]\n",
        "\n",
        "    # Get feedback as targets\n",
        "    targets = examples[\"answer_feedback\"]\n",
        "\n",
        "    # Tokenize the inputs\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "    # Tokenize the targets\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            targets,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=256,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    return model_inputs\n",
        "\n",
        "# Apply preprocessing to the datasets\n",
        "tokenized_train = dataset[\"train\"].map(preprocess_function, batched=True, remove_columns=dataset[\"train\"].column_names)\n",
        "tokenized_val = dataset[\"validation\"].map(preprocess_function, batched=True, remove_columns=dataset[\"validation\"].column_names)\n",
        "tokenized_test = dataset[\"test_unseen_answers\"].map(preprocess_function, batched=True, remove_columns=dataset[\"test_unseen_answers\"].column_names)\n",
        "\n",
        "# Set the format for PyTorch\n",
        "tokenized_train.set_format(\"torch\")\n",
        "tokenized_val.set_format(\"torch\")\n",
        "tokenized_test.set_format(\"torch\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y-dmxiRXvZzK"
      },
      "source": [
        "## Evaluation metric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "PNz9qpRCvbZF"
      },
      "outputs": [],
      "source": [
        "# Load the ROUGE and BLEU metrics\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "\n",
        "    # Decode predicted and reference texts\n",
        "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
        "\n",
        "    # Replace -100 in the labels as we can't decode them\n",
        "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "    # ROUGE expects a newline after each sentence\n",
        "    decoded_preds = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
        "    decoded_labels = [\"\\n\".join(sent_tokenize(label.strip())) for label in decoded_labels]\n",
        "\n",
        "    # Calculate ROUGE scores\n",
        "    result = rouge.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
        "\n",
        "    # Calculate BLEU score\n",
        "    result[\"bleu\"] = bleu.compute(predictions=decoded_preds, references=[[label] for label in decoded_labels])[\"bleu\"]\n",
        "\n",
        "    # Add mean generated length\n",
        "    prediction_lens = [len(pred.split()) for pred in decoded_preds]\n",
        "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
        "\n",
        "    return {k: round(v, 4) for k, v in result.items()}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwVvzUcGnhAf"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAVDZQIFrhTG",
        "outputId": "5e81f854-1f3d-4c5c-9d71-cde6926eecaa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model is on device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "# Load the model and tokenizer\n",
        "model_name = \"facebook/bart-large-cnn\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "# Verify the model is on the expected device\n",
        "print(f\"Model is on device: {next(model.parameters()).device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "QfbUugxNndXX",
        "outputId": "1871b31b-b8d8-4e7b-838e-e9b28e465a9a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>▁█</td></tr><tr><td>eval/gen_len</td><td>█▁</td></tr><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/rouge1</td><td>▁█</td></tr><tr><td>eval/rouge2</td><td>▁█</td></tr><tr><td>eval/rougeL</td><td>▁█</td></tr><tr><td>eval/rougeLsum</td><td>▁█</td></tr><tr><td>eval/runtime</td><td>▁█</td></tr><tr><td>eval/samples_per_second</td><td>█▁</td></tr><tr><td>eval/steps_per_second</td><td>▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▃▄▄▄▅▅▆▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▃▄▄▄▅▅▆▆▆▇▇███</td></tr><tr><td>train/grad_norm</td><td>▂█▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>██▇▇▆▆▆▅▅▄▄▃▃▃▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▇▄▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>0.0593</td></tr><tr><td>eval/gen_len</td><td>52.7822</td></tr><tr><td>eval/loss</td><td>0.95482</td></tr><tr><td>eval/rouge1</td><td>0.2567</td></tr><tr><td>eval/rouge2</td><td>0.1195</td></tr><tr><td>eval/rougeL</td><td>0.2055</td></tr><tr><td>eval/rougeLsum</td><td>0.2282</td></tr><tr><td>eval/runtime</td><td>237.572</td></tr><tr><td>eval/samples_per_second</td><td>1.797</td></tr><tr><td>eval/steps_per_second</td><td>0.45</td></tr><tr><td>train/epoch</td><td>2</td></tr><tr><td>train/global_step</td><td>850</td></tr><tr><td>train/grad_norm</td><td>1.78879</td></tr><tr><td>train/learning_rate</td><td>4e-05</td></tr><tr><td>train/loss</td><td>1.1599</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">ancient-universe-33</strong> at: <a href='https://wandb.ai/mani696701-northeastern-university/uncategorized/runs/mf0mjf4d' target=\"_blank\">https://wandb.ai/mani696701-northeastern-university/uncategorized/runs/mf0mjf4d</a><br> View project at: <a href='https://wandb.ai/mani696701-northeastern-university/uncategorized' target=\"_blank\">https://wandb.ai/mani696701-northeastern-university/uncategorized</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250412_164937-mf0mjf4d/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250412_170235-07wf9mqz</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mani696701-northeastern-university/uncategorized/runs/07wf9mqz' target=\"_blank\">twilight-sea-34</a></strong> to <a href='https://wandb.ai/mani696701-northeastern-university/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/mani696701-northeastern-university/uncategorized' target=\"_blank\">https://wandb.ai/mani696701-northeastern-university/uncategorized</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/mani696701-northeastern-university/uncategorized/runs/07wf9mqz' target=\"_blank\">https://wandb.ai/mani696701-northeastern-university/uncategorized/runs/07wf9mqz</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "trainable params: 2,359,296 || all params: 408,649,728 || trainable%: 0.5773\n"
          ]
        }
      ],
      "source": [
        "# Initialize W&B\n",
        "wandb.login(key=\"\")\n",
        "wandb.init()\n",
        "\n",
        "# Define LoRA configuration for BART\n",
        "peft_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "    inference_mode=False,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\"]  # Target attention layers in BART\n",
        ")\n",
        "\n",
        "# Get the PEFT model\n",
        "model = get_peft_model(model, peft_config)\n",
        "model.print_trainable_parameters()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zihe0fIswBsf"
      },
      "source": [
        "## PEFT Fine-tuning using Lora"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "REG-O9AMwAGh",
        "outputId": "23448818-38d6-47f0-97fd-c6c6833c71c2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n",
            "<ipython-input-19-4cd7848b13eb>:34: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Seq2SeqTrainer(\n",
            "No label_names provided for model class `PeftModelForSeq2SeqLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2130' max='2130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [2130/2130 31:01, Epoch 10/10]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Rouge1</th>\n",
              "      <th>Rouge2</th>\n",
              "      <th>Rougel</th>\n",
              "      <th>Rougelsum</th>\n",
              "      <th>Bleu</th>\n",
              "      <th>Gen Len</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.433800</td>\n",
              "      <td>1.051032</td>\n",
              "      <td>0.232200</td>\n",
              "      <td>0.082200</td>\n",
              "      <td>0.178900</td>\n",
              "      <td>0.201300</td>\n",
              "      <td>0.034800</td>\n",
              "      <td>51.522200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.268300</td>\n",
              "      <td>0.987097</td>\n",
              "      <td>0.242800</td>\n",
              "      <td>0.103800</td>\n",
              "      <td>0.194500</td>\n",
              "      <td>0.214600</td>\n",
              "      <td>0.046800</td>\n",
              "      <td>53.278700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.196300</td>\n",
              "      <td>0.962763</td>\n",
              "      <td>0.258900</td>\n",
              "      <td>0.120100</td>\n",
              "      <td>0.208500</td>\n",
              "      <td>0.229800</td>\n",
              "      <td>0.059600</td>\n",
              "      <td>52.695600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.161800</td>\n",
              "      <td>0.946353</td>\n",
              "      <td>0.263300</td>\n",
              "      <td>0.128300</td>\n",
              "      <td>0.214300</td>\n",
              "      <td>0.233500</td>\n",
              "      <td>0.065900</td>\n",
              "      <td>52.288100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.151500</td>\n",
              "      <td>0.937312</td>\n",
              "      <td>0.269100</td>\n",
              "      <td>0.136700</td>\n",
              "      <td>0.220100</td>\n",
              "      <td>0.239900</td>\n",
              "      <td>0.074700</td>\n",
              "      <td>52.838400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.142700</td>\n",
              "      <td>0.929586</td>\n",
              "      <td>0.271800</td>\n",
              "      <td>0.141800</td>\n",
              "      <td>0.226100</td>\n",
              "      <td>0.245800</td>\n",
              "      <td>0.078700</td>\n",
              "      <td>51.913300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.135800</td>\n",
              "      <td>0.926046</td>\n",
              "      <td>0.273100</td>\n",
              "      <td>0.144100</td>\n",
              "      <td>0.226100</td>\n",
              "      <td>0.246300</td>\n",
              "      <td>0.081200</td>\n",
              "      <td>51.915700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.136700</td>\n",
              "      <td>0.922754</td>\n",
              "      <td>0.275800</td>\n",
              "      <td>0.149800</td>\n",
              "      <td>0.227700</td>\n",
              "      <td>0.250300</td>\n",
              "      <td>0.085900</td>\n",
              "      <td>52.861800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.138000</td>\n",
              "      <td>0.921732</td>\n",
              "      <td>0.274900</td>\n",
              "      <td>0.148900</td>\n",
              "      <td>0.227200</td>\n",
              "      <td>0.248600</td>\n",
              "      <td>0.084200</td>\n",
              "      <td>53.281000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.116700</td>\n",
              "      <td>0.920515</td>\n",
              "      <td>0.275900</td>\n",
              "      <td>0.148100</td>\n",
              "      <td>0.227600</td>\n",
              "      <td>0.248600</td>\n",
              "      <td>0.084600</td>\n",
              "      <td>52.955500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>▁▃▄▅▆▇▇███</td></tr><tr><td>eval/gen_len</td><td>▁█▆▄▆▃▃▆█▇</td></tr><tr><td>eval/loss</td><td>█▅▃▂▂▁▁▁▁▁</td></tr><tr><td>eval/rouge1</td><td>▁▃▅▆▇▇████</td></tr><tr><td>eval/rouge2</td><td>▁▃▅▆▇▇▇███</td></tr><tr><td>eval/rougeL</td><td>▁▃▅▆▇█████</td></tr><tr><td>eval/rougeLsum</td><td>▁▃▅▆▇▇▇███</td></tr><tr><td>eval/runtime</td><td>▁▅▅▄█▂▁▃▆▆</td></tr><tr><td>eval/samples_per_second</td><td>▇▄▄▅▁▇█▆▃▃</td></tr><tr><td>eval/steps_per_second</td><td>▇▄▄▅▁▇█▆▃▃</td></tr><tr><td>train/epoch</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train/grad_norm</td><td>█▇▅▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▂▂▁▁▁▁▂▂▁▂▁▁</td></tr><tr><td>train/learning_rate</td><td>████▇▇▇▇▇▇▆▆▆▆▆▅▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▁▁</td></tr><tr><td>train/loss</td><td>█▇▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/bleu</td><td>0.0846</td></tr><tr><td>eval/gen_len</td><td>52.9555</td></tr><tr><td>eval/loss</td><td>0.92052</td></tr><tr><td>eval/rouge1</td><td>0.2759</td></tr><tr><td>eval/rouge2</td><td>0.1481</td></tr><tr><td>eval/rougeL</td><td>0.2276</td></tr><tr><td>eval/rougeLsum</td><td>0.2486</td></tr><tr><td>eval/runtime</td><td>143.9361</td></tr><tr><td>eval/samples_per_second</td><td>2.967</td></tr><tr><td>eval/steps_per_second</td><td>0.375</td></tr><tr><td>total_flos</td><td>1.8543600992256e+16</td></tr><tr><td>train/epoch</td><td>10</td></tr><tr><td>train/global_step</td><td>2130</td></tr><tr><td>train/grad_norm</td><td>0.57798</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.1167</td></tr><tr><td>train_loss</td><td>1.47213</td></tr><tr><td>train_runtime</td><td>1861.7336</td></tr><tr><td>train_samples_per_second</td><td>9.131</td></tr><tr><td>train_steps_per_second</td><td>1.144</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">twilight-sea-34</strong> at: <a href='https://wandb.ai/mani696701-northeastern-university/uncategorized/runs/07wf9mqz' target=\"_blank\">https://wandb.ai/mani696701-northeastern-university/uncategorized/runs/07wf9mqz</a><br> View project at: <a href='https://wandb.ai/mani696701-northeastern-university/uncategorized' target=\"_blank\">https://wandb.ai/mani696701-northeastern-university/uncategorized</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20250412_170235-07wf9mqz/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Define data collator\n",
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer,\n",
        "    model=model,\n",
        "    label_pad_token_id=-100,\n",
        "    pad_to_multiple_of=8\n",
        ")\n",
        "\n",
        "# Define training arguments\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./results/bart-student-feedback\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=3,\n",
        "    num_train_epochs=10,\n",
        "    predict_with_generate=True,\n",
        "    fp16=True,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"rouge1\",\n",
        "    greater_is_better=True,\n",
        "    push_to_hub=False,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_strategy=\"steps\",\n",
        "    logging_steps=50,\n",
        "    logging_first_step=True,\n",
        "    report_to=\"wandb\",\n",
        ")\n",
        "\n",
        "# Create Seq2SeqTrainer instance\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_train,\n",
        "    eval_dataset=tokenized_val,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "trainer.train()\n",
        "\n",
        "# Stop W&B\n",
        "wandb.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vaele7HbyB6C"
      },
      "source": [
        "## Saving model adapters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9OWHEYaT4Lmt",
        "outputId": "f9d7bf85-efca-489c-cee8-39469ad7adf3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "('./results/bart-student-feedback/final/tokenizer_config.json',\n",
              " './results/bart-student-feedback/final/special_tokens_map.json',\n",
              " './results/bart-student-feedback/final/vocab.json',\n",
              " './results/bart-student-feedback/final/merges.txt',\n",
              " './results/bart-student-feedback/final/added_tokens.json',\n",
              " './results/bart-student-feedback/final/tokenizer.json')"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Save the model and adapters\n",
        "model_save_path = \"./results/bart-student-feedback/final\"\n",
        "model.save_pretrained(model_save_path)\n",
        "tokenizer.save_pretrained(model_save_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X6wSMOeayIle"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0G-lkE6F4i9",
        "outputId": "e3dc5da7-19d1-44d0-d548-c6671d738fd2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Custom Question:\n",
            "What is the difference between TCP and UDP protocols?\n",
            "\n",
            "Student Answer:\n",
            "TCP is connection-oriented and guarantees delivery of packets, while UDP is connectionless and doesn't guarantee delivery.\n",
            "\n",
            "Score:\n",
            "0.8\n",
            "\n",
            "Generated Feedback:\n",
            "The response is partially correct as it states the difference between TCP and UDP but does not explain why TCP is connection-oriented and UDP is connectionless. However, the explanation of the difference is not correct as TCP is not a connectionless protocol, it is just a TCP-based protocol.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from peft import PeftModel, PeftConfig\n",
        "\n",
        "# Path to your saved model\n",
        "model_save_path = \"./results/bart-student-feedback/final\"\n",
        "\n",
        "# Load the PEFT configuration\n",
        "config = PeftConfig.from_pretrained(model_save_path)\n",
        "\n",
        "# Load the base model and tokenizer\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "# Load the PEFT adapter weights\n",
        "model = PeftModel.from_pretrained(model, model_save_path)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Custom test question and answer\n",
        "custom_question = \"What is the difference between TCP and UDP protocols?\"\n",
        "custom_answer = \"TCP is connection-oriented and guarantees delivery of packets, while UDP is connectionless and doesn't guarantee delivery.\"\n",
        "custom_score = 0.8  # Assuming a score between 0 and 1\n",
        "\n",
        "# Generate feedback\n",
        "def generate_feedback(question, answer, score):\n",
        "    input_text = f\"Question: {question}\\nStudent Answer: {answer}\\nScore: {score}\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    # Move inputs to the same device as model\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Generate feedback\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_length=256,\n",
        "            num_beams=4,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    # Decode the generated feedback\n",
        "    generated_feedback = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return generated_feedback\n",
        "\n",
        "# Generate and print feedback\n",
        "feedback = generate_feedback(custom_question, custom_answer, custom_score)\n",
        "\n",
        "print(\"Custom Question:\")\n",
        "print(custom_question)\n",
        "print(\"\\nStudent Answer:\")\n",
        "print(custom_answer)\n",
        "print(\"\\nScore:\")\n",
        "print(custom_score)\n",
        "print(\"\\nGenerated Feedback:\")\n",
        "print(feedback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gv2T4NFRGRPu",
        "outputId": "395ba6e8-a6ee-4c49-cf89-7edac2317202"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Test Question:\n",
            "State at least 4 of the differences shown in the lecture between the UDP and TCP headers.\n",
            "\n",
            "Student Answer:\n",
            "In TCP there is a Sequence Number field to identify packets individually for reliability. There is no Sequence Number in UDP. The UDP header does not have an options field, while the TCP header does. In TCP there is an Advertised Window field for the Sliding Window Protocol for Flow Control. There is no Flow Control and therefore no Advertised Window field in UDP. In TCP there there is only a Data Offset field that specifies the header length. In UDP the whole Packet Length is transmitted.\n",
            "\n",
            "Score:\n",
            "1.0\n",
            "\n",
            "True Feedback:\n",
            "The response correctly identifies four differences between TCP and UDP headers.\n",
            "\n",
            "Generated Feedback:\n",
            "The response correctly states four differences between TCP and UDP headers. The response also correctly identifies the Advertised Window field in TCP and the Data Offset field in UDP. Apart from that, the response is correct as it correctly identifies all four differences in the difference between the two headers.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from peft import PeftModel, PeftConfig\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"Short-Answer-Feedback/saf_communication_networks_english\")\n",
        "\n",
        "# Path to your saved model\n",
        "model_save_path = \"./results/bart-student-feedback/final\"\n",
        "\n",
        "# Load the PEFT configuration\n",
        "config = PeftConfig.from_pretrained(model_save_path)\n",
        "\n",
        "# Load the base model and tokenizer\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "# Load the PEFT adapter weights\n",
        "model = PeftModel.from_pretrained(model, model_save_path)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Choose an example from the test set\n",
        "test_example = dataset[\"test_unseen_answers\"][0]  # Using the first example\n",
        "question = test_example[\"question\"]\n",
        "answer = test_example[\"provided_answer\"]\n",
        "score = test_example[\"score\"]\n",
        "true_feedback = test_example[\"answer_feedback\"]\n",
        "\n",
        "# Generate feedback\n",
        "def generate_feedback(question, answer, score):\n",
        "    input_text = f\"Question: {question}\\nStudent Answer: {answer}\\nScore: {score}\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    # Move inputs to the same device as model\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Generate feedback\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_length=256,\n",
        "            num_beams=4,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    # Decode the generated feedback\n",
        "    generated_feedback = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return generated_feedback\n",
        "\n",
        "# Generate and print feedback\n",
        "feedback = generate_feedback(question, answer, score)\n",
        "\n",
        "print(\"Test Question:\")\n",
        "print(question)\n",
        "print(\"\\nStudent Answer:\")\n",
        "print(answer)\n",
        "print(\"\\nScore:\")\n",
        "print(score)\n",
        "print(\"\\nTrue Feedback:\")\n",
        "print(true_feedback)\n",
        "print(\"\\nGenerated Feedback:\")\n",
        "print(feedback)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9gN_TR5Gn7b",
        "outputId": "d4208f4d-dc56-47e5-e789-c93d44460833"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Example 1 (index 327) ---\n",
            "Question:\n",
            "Name the 3 service classes the Data Link Layer offers and explain the differences between the classes.\n",
            "\n",
            "Student Answer:\n",
            "- Unconfirmed Connectionless Service \n",
            "The Unconfirmed Connectionless Service sends data to the receiver, without announcing it (building up a connection) first in data frames without any flow control. Because of the missing connection and flow control, it is possible that complete data frames can get lost.  \n",
            "- Confirmed Connectionless Service \n",
            "Wheras the confirmed connectionless service sends the data frames and waits for an acknowledgement of the corresponding recipient. If the recipient confirms the data frame, the next data frame is being sent. If the recipient doesn’t answer for a long time, the data frame is being resent. If for some reason, the ackknowledgement gets lost, the recipient will eventually get a data frame twice, and will not be able to detect the duplication. The correction has to be made on a higher level. It is much slower than the unconfirmed, because of waittime for timeouts and ackknowledgement messages. \n",
            "- Connection Oriented Service \n",
            "In the connection oriented service, the overhead is a lot higher, but the advantages are a detailed flow control, in which the recipient can detect duplicates, ask for a certain frame and can align the frames in the right order. And if the recipient reads slower than the sender transmit, it is possible to make a transmission. The participants first exchange a handshake and afterwards are transferring data. Afterwards the connection is disconnected.\n",
            "\n",
            "Score:\n",
            "1.0\n",
            "\n",
            "True Feedback:\n",
            "The response answers the services' names and differences correctly.\n",
            "\n",
            "Generated Feedback:\n",
            "The response answers the services' names and differences correctly. The response also explains the differences between the services with different service types and the explanation of the differences is correct as it is very accurate and complete. The connection Oriented Service has a higher overhead, but the advantages are a detailed flow control and a more detailed detection of duplicates.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--- Example 2 (index 57) ---\n",
            "Question:\n",
            "Let us assume that you flip a coin 6 times where the probability of heads (H) showing up is 0.6. Please arrange the following events in the increasing order of their likelihood (i.e., least probable → most probable): ● Event A: you see at least three H’s ● Event B: you see the sequence HHHTTT ● Event C: you see exactly three H’s Include justification in your answer headers.\n",
            "\n",
            "Student Answer:\n",
            "By calculating using the binomial distribution probability formula, it can be concluded that the probability of event C occurring is 0.2765. \n",
            "The probability of event A occurring is: P[event A]=P[you see exactly three H’s]+P[you see four H’s]+P[you see five H’s]+P[you see six H’s] =P(k=3)+P(k=4)+P(k=5)+P(k=6)= 0.2765+0.311+0.1866+0.0467=0.8208 (P is the binomial distribution)\n",
            "The probability of event B occurring is 0.6*0.6*0.6*0.4*0.4*0.4=0.013824. \n",
            "Therefore the order is event B to event C to event A.\n",
            "\n",
            "Score:\n",
            "1.0\n",
            "\n",
            "True Feedback:\n",
            "The response correctly answers the order of the events with appropriate justification.\n",
            "\n",
            "Generated Feedback:\n",
            "The response correctly answers the order of the events in the binomial distribution with justification. The explanation for the order is correct as it is based on the probability of occurrence of each event occurring in the order in which it occurs. The order of events A, B, C and D is correct.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--- Example 3 (index 12) ---\n",
            "Question:\n",
            "What is the “Dynamic Host Configuration Protocol (DHCP)”? What is it used for? \n",
            "\n",
            "Student Answer:\n",
            "DHCP extends the functionality of RARP. It is used for automatic IP address assignment.\n",
            "\n",
            "Score:\n",
            "0.5\n",
            "\n",
            "True Feedback:\n",
            "The response only states the definition/description of DHCP correctly. The usage is missing in the response.\n",
            "\n",
            "Generated Feedback:\n",
            "The response answers the definition and the usage of DHCP correctly. However, the description of DHCP is partially correct as it does not mention the use of RARP in the definition. The use of DHCP for automatic IP address assignment is incorrect as RARP is not used for automatic address assignment.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--- Example 4 (index 140) ---\n",
            "Question:\n",
            "To model the packet arrivals as a poisson process, we assumed that the arrivals for each time interval Δt are independent. Does this assumption hold for real INTERNET traffic? Explain your answer in 2-5 sentences. \n",
            "\n",
            "Student Answer:\n",
            "No, this assumption will most likely not hold true for real internet traffic. This has multiple reasons:\n",
            "\n",
            "Packets on the internet are grouped into frames for sending, making lone packets being sent separately rather unlikely.  \n",
            "The nature of data transfer on the internet also makes lone packets very unlikely. When making a request for data through the internet (for example loading a web page), the response includes a lot of data (markup, text, images) which are all sent in a short amount of time, and after the page has been loaded the user will most likely spend some time browsing the page before making another request.  \n",
            "Therefore we cannot treat arriving packets as independent from one another, because there is a very high chance that an arriving packet is related to the previous packet.\n",
            "\n",
            "Score:\n",
            "1.0\n",
            "\n",
            "True Feedback:\n",
            "The response is correct as it correctly associates the probability of a packet arrival happening at a node with previous arrivals at the node.\n",
            "\n",
            "Generated Feedback:\n",
            "The response is correct as it correctly associates the poisson process with the probability of arriving packets being related to the previous arrivals. The response is partially correct as the assumption does not hold true for real internet traffic as there is a high chance that an arriving packet is related to its previous arrival.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--- Example 5 (index 125) ---\n",
            "Question:\n",
            "WHAT is the purpose of Reverse Path Forwarding and Reverse Path Broadcast? HOW do they work?\n",
            "\n",
            "Student Answer:\n",
            "Reverse Path Forwarding: - Purpose: reduce traffic in broadcasting compared to flooding. In Reverse Path Forwarding, a sender only sends an incoming packet to all of its adjacent nodes if it has arrived over the edge that is considered to be part of the shortest path between that node and the source. Otherwise, the packet is ignored. Reverse Path Broadcasting: - Purpose: further reduce traffic compared to Reverse Path Forwarding. In Reverse Path Broadcasting, if a packet has arrived over the edge which is usually used for sending packets to the source, it is only forwarded to those neighbors, which usually route unicast messages to the sender via that node. So a router only spreads packets to a neighbor if it is on the shortest path between that neighbor and the source.\n",
            "\n",
            "Score:\n",
            "1.0\n",
            "\n",
            "True Feedback:\n",
            "The response correctly answers all three parts of the question. However, the purpose of minimizing the number of duplicate packets in the network is not explicitly stated in the response.\n",
            "\n",
            "Generated Feedback:\n",
            "The response answers the purpose and the description of RPAF and RPB correctly explains the purpose of the systems and how they work. The explanation of RPB is partially correct as it does not mention what happens to the sender if a packet arrives over the edge of the broadcast edge.\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import random\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from peft import PeftModel, PeftConfig\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"Short-Answer-Feedback/saf_communication_networks_english\")\n",
        "\n",
        "# Path to your saved model\n",
        "model_save_path = \"./results/bart-student-feedback/final\"\n",
        "\n",
        "# Load the PEFT configuration\n",
        "config = PeftConfig.from_pretrained(model_save_path)\n",
        "\n",
        "# Load the base model and tokenizer\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "# Load the PEFT adapter weights\n",
        "model = PeftModel.from_pretrained(model, model_save_path)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Generate feedback\n",
        "def generate_feedback(question, answer, score):\n",
        "    input_text = f\"Question: {question}\\nStudent Answer: {answer}\\nScore: {score}\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    # Move inputs to the same device as model\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Generate feedback\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_length=256,\n",
        "            num_beams=4,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    # Decode the generated feedback\n",
        "    generated_feedback = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return generated_feedback\n",
        "\n",
        "# Get 5 random examples from the test set\n",
        "test_set_size = len(dataset[\"test_unseen_answers\"])\n",
        "random_indices = random.sample(range(test_set_size), 5)\n",
        "\n",
        "# Generate and print feedback for each random example\n",
        "for i, idx in enumerate(random_indices):\n",
        "    test_example = dataset[\"test_unseen_answers\"][idx]\n",
        "    question = test_example[\"question\"]\n",
        "    answer = test_example[\"provided_answer\"]\n",
        "    score = test_example[\"score\"]\n",
        "    true_feedback = test_example[\"answer_feedback\"]\n",
        "\n",
        "    # Generate feedback\n",
        "    feedback = generate_feedback(question, answer, score)\n",
        "\n",
        "    print(f\"\\n--- Example {i+1} (index {idx}) ---\")\n",
        "    print(\"Question:\")\n",
        "    print(question)\n",
        "    print(\"\\nStudent Answer:\")\n",
        "    print(answer)\n",
        "    print(\"\\nScore:\")\n",
        "    print(score)\n",
        "    print(\"\\nTrue Feedback:\")\n",
        "    print(true_feedback)\n",
        "    print(\"\\nGenerated Feedback:\")\n",
        "    print(feedback)\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8hGXRmWHdNl",
        "outputId": "578be251-0ec9-4f27-b4b0-8b5a4e38a3f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Example 1 (index 114) ---\n",
            "Question:\n",
            "Consider a single server queueing system with a buffer of size 10. Let us assume that 9 packets arrive per second and 10 packets are served per second on an average. Assume you monitor the system for exactly one minute after the system reaches equilibrium. How many seconds would you expect the system to be in a state in which there are less than 10 packets waiting in the queue? You need to justify your answer by showing steps involved; calculations, however, need not be included. headers.\n",
            "\n",
            "Student Answer:\n",
            "Since the system reached an equilibrium the probabilities do not change anymore ( dp_n(t)/dt = 0 ). So it can be assumed that the queue is emptied by one package each second on average (10 served - 9 arrived).\n",
            "\n",
            "Now assuming that the queue is full at obvservation start, it will be empty after 10 seconds, from which 9 seconds the queue has less packets then 10.\n",
            "\n",
            "Score:\n",
            "0.0\n",
            "\n",
            "True Feedback:\n",
            "The response is incorrect because the stated number of expected seconds is incorrect as the correct number of seconds is 56.95 seconds.\n",
            "\n",
            "Generated Feedback:\n",
            "The response is partially correct as it does not provide a justification for calculating the time it takes for the queue to be empty after a packet is served. However, the stated time is correct, but the explanation for the calculation is not correct because it is not stated how long it takes to empty the queue after each packet arrival.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--- Example 2 (index 71) ---\n",
            "Question:\n",
            "Assume you have a local network with 3 users that are all interconnected and have perfect clocks. Typically the network is often congested as all users generate more traffic than the link’s capacities. Which of the encoding techniques introduced in the lecture should be used in this network to encode bitstreams? Give two reasons for your answer in 2-4 sentences.\n",
            "\n",
            "Student Answer:\n",
            "I would use Binary Encoding. It is efficient since it uses 1 bit per baud. It has no self-clocking feature but that is not needed since all user have perfect clocks.\n",
            "\n",
            "Score:\n",
            "2.5\n",
            "\n",
            "True Feedback:\n",
            "Correct.\n",
            "\n",
            "Generated Feedback:\n",
            "The response correctly answers both the two reasons for using binary encoding and the correct choice of encoding in this case. The response is correct as it correctly identifies the binary encoding as an efficient encoding method and provides two reasons why it is not needed in the case of a congested network.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--- Example 3 (index 52) ---\n",
            "Question:\n",
            "What requirement has to be met so that you can use the piggybacking extension to the sliding window protocol?\n",
            "\n",
            "Student Answer:\n",
            "Requirement: The interval of two adjacent frames, which are sent by sender, is short.\n",
            "So that we can use piggybacking to response these two frames with one acknowledgement. \n",
            "The communication has to be duplex (so the protocol must not be \"Utopia\").\n",
            "And the receiving buffer from the Sender must be ,so that it is able to store the ACK plus the additional data!\n",
            "\n",
            "Score:\n",
            "0.5\n",
            "\n",
            "True Feedback:\n",
            "The response contains a duplex connection as one of the requirements, but having to send two frames within short intervals is incorrect. Also, the same data and acknowledgments are tied together in piggybacking. Therefore, the total buffer space requirement should ideally remain almost the same as when they are sent separately.\n",
            "\n",
            "Generated Feedback:\n",
            "The response is partially correct as it does not mention the duplex protocol requirement. However, the response does not state the reason for using piggybacking in the sliding window protocol. Also, the receiving buffer from the Sender must be short so that it is able to store the ACK plus the additional data.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--- Example 4 (index 346) ---\n",
            "Question:\n",
            "Please explain the problem with \"Distributed Queue Dual Buses\" that was discussed in the lecture in 1-3 sentences.\n",
            "\n",
            "Student Answer:\n",
            "The Distributed Queue Dual Bus (DQDB) architecture uses two unidirectional buses for sending and receiving data. The main challenge here ist to guarentee fairness between all participating nodes as different nodes may have advantages (if at the beginning of the bus) or disadvantages (if at the end of the bus) in write access depending on their position in the bus.\n",
            "\n",
            "Score:\n",
            "1.0\n",
            "\n",
            "True Feedback:\n",
            "The response correctly states and explains the fairness problem of reserving transmission right in DQDB.\n",
            "\n",
            "Generated Feedback:\n",
            "The response is correct as it correctly identifies the problem of fairness in DQDB and provides an explanation for it. The response also provides a correct description of the fairness problem in the distributed bus architecture with two unidirectional buses for sending and receiving data.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "--- Example 5 (index 279) ---\n",
            "Question:\n",
            "Consider the following topology from the exercise. This time, node A wants to distribute a packet using Reverse Path Broadcast (RPB). Assume that every IS knows the best path to A and also whether they are the next hop of their neighbors on the unicast path to A.Please list all the packets which are sent together with the information whether they will be forwarded or dropped at the receiving nodes. Use the following notation: (sender, receiver, drop) for dropped and (sender, receiver, forward) for forwarded packets. Please group these tuples according to the number of hops the packets have travelled so far. For dropped packets, please specify the reason why the packet has been dropped in a few words.Example for the notation:Hop 1:(C, A, forward)(C, B, drop) <= describe reason hereHop 2:(A, D, drop) <= describe reason here\n",
            "\n",
            "Student Answer:\n",
            "Hop 1:\n",
            "(A, B, forward)\n",
            "(A, C, forward)\n",
            "(A, D, drop): Because D recognize that F and C won't receive packets via D.\n",
            "\n",
            "\n",
            "Hop 2:\n",
            "(B, E, forward)\n",
            "(C, F, drop): Because F recognize that E,D and G won't receive packets via F.\n",
            "\n",
            "\n",
            "Hop 3:\n",
            "(E, G, forward)\n",
            "\n",
            "\n",
            "Hop 4:\n",
            "(G, H, drop): There is only one possibility for  H to receive the packet (via G ) and it can't be send it anywhere else.\n",
            "\n",
            "Score:\n",
            "2.5\n",
            "\n",
            "True Feedback:\n",
            "The response is correct.\n",
            "\n",
            "Generated Feedback:\n",
            "The response is correct as it correctly states the underlying topology and provides a suitable reason for dropping a packet with (A, D, drop) and forwarding it to the receiving node. The remaining part of the response is partially correct as the response does not mention why the packet will be forwarded or dropped.\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import random\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from peft import PeftModel, PeftConfig\n",
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"Short-Answer-Feedback/saf_communication_networks_english\")\n",
        "\n",
        "# Path to your saved model\n",
        "model_save_path = \"./results/bart-student-feedback/final\"\n",
        "\n",
        "# Load the PEFT configuration\n",
        "config = PeftConfig.from_pretrained(model_save_path)\n",
        "\n",
        "# Load the base model and tokenizer\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "# Load the PEFT adapter weights\n",
        "model = PeftModel.from_pretrained(model, model_save_path)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Generate feedback\n",
        "def generate_feedback(question, answer, score):\n",
        "    input_text = f\"Question: {question}\\nStudent Answer: {answer}\\nScore: {score}\"\n",
        "    inputs = tokenizer(input_text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    # Move inputs to the same device as model\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Generate feedback\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            **inputs,\n",
        "            max_length=256,\n",
        "            num_beams=4,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    # Decode the generated feedback\n",
        "    generated_feedback = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "    return generated_feedback\n",
        "\n",
        "# Get 5 random examples from the test set\n",
        "test_set_size = len(dataset[\"test_unseen_answers\"])\n",
        "random_indices = random.sample(range(test_set_size), 5)\n",
        "\n",
        "# Generate and print feedback for each random example\n",
        "for i, idx in enumerate(random_indices):\n",
        "    test_example = dataset[\"test_unseen_answers\"][idx]\n",
        "    question = test_example[\"question\"]\n",
        "    answer = test_example[\"provided_answer\"]\n",
        "    score = test_example[\"score\"]\n",
        "    true_feedback = test_example[\"answer_feedback\"]\n",
        "\n",
        "    # Generate feedback\n",
        "    feedback = generate_feedback(question, answer, score)\n",
        "\n",
        "    print(f\"\\n--- Example {i+1} (index {idx}) ---\")\n",
        "    print(\"Question:\")\n",
        "    print(question)\n",
        "    print(\"\\nStudent Answer:\")\n",
        "    print(answer)\n",
        "    print(\"\\nScore:\")\n",
        "    print(score)\n",
        "    print(\"\\nTrue Feedback:\")\n",
        "    print(true_feedback)\n",
        "    print(\"\\nGenerated Feedback:\")\n",
        "    print(feedback)\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3rAAh0AcGxt_",
        "outputId": "8481d5d7-8909-4996-b851-2f3ee313c2ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Generating predictions for 375 examples in 47 batches...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 47/47 [02:02<00:00,  2.61s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluation Results on Test Set:\n",
            "ROUGE-1: 28.14%\n",
            "ROUGE-2: 15.81%\n",
            "ROUGE-L: 23.54%\n",
            "BLEU: 9.02%\n",
            "\n",
            "Additional Statistics:\n",
            "Average prediction length: 53.88 words\n",
            "Average reference length: 17.16 words\n",
            "Number of test examples: 375\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "from peft import PeftModel, PeftConfig\n",
        "from datasets import load_dataset\n",
        "import evaluate\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "# Load metrics\n",
        "rouge = evaluate.load(\"rouge\")\n",
        "bleu = evaluate.load(\"bleu\")\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_dataset(\"Short-Answer-Feedback/saf_communication_networks_english\")\n",
        "test_dataset = dataset[\"test_unseen_answers\"]\n",
        "\n",
        "# Path to your saved model\n",
        "model_save_path = \"./results/bart-student-feedback/final\"\n",
        "\n",
        "# Load the PEFT configuration\n",
        "config = PeftConfig.from_pretrained(model_save_path)\n",
        "\n",
        "# Load the base model and tokenizer\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(config.base_model_name_or_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(config.base_model_name_or_path)\n",
        "\n",
        "# Load the PEFT adapter weights\n",
        "model = PeftModel.from_pretrained(model, model_save_path)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "# Process examples in batches\n",
        "def process_batch(batch_examples, batch_size=8):\n",
        "    questions = [example[\"question\"] for example in batch_examples]\n",
        "    answers = [example[\"provided_answer\"] for example in batch_examples]\n",
        "    scores = [example[\"score\"] for example in batch_examples]\n",
        "    true_feedbacks = [example[\"answer_feedback\"] for example in batch_examples]\n",
        "\n",
        "    # Generate inputs for all examples in the batch\n",
        "    batch_inputs = []\n",
        "    for q, a, s in zip(questions, answers, scores):\n",
        "        input_text = f\"Question: {q}\\nStudent Answer: {a}\\nScore: {s}\"\n",
        "        batch_inputs.append(input_text)\n",
        "\n",
        "    # Tokenize all inputs\n",
        "    tokenized_inputs = tokenizer(\n",
        "        batch_inputs,\n",
        "        padding=True,\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        return_tensors=\"pt\"\n",
        "    ).to(device)\n",
        "\n",
        "    # Generate all outputs\n",
        "    with torch.no_grad():\n",
        "        output_ids = model.generate(\n",
        "            input_ids=tokenized_inputs.input_ids,\n",
        "            attention_mask=tokenized_inputs.attention_mask,\n",
        "            max_length=256,\n",
        "            num_beams=4,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    # Decode all outputs\n",
        "    generated_feedbacks = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
        "\n",
        "    return generated_feedbacks, true_feedbacks\n",
        "\n",
        "# Generate predictions for the entire test set in batches\n",
        "BATCH_SIZE = 8\n",
        "all_predictions = []\n",
        "all_references = []\n",
        "\n",
        "# Calculate number of batches\n",
        "num_examples = len(test_dataset)\n",
        "num_batches = (num_examples + BATCH_SIZE - 1) // BATCH_SIZE  # Ceiling division\n",
        "\n",
        "print(f\"Generating predictions for {num_examples} examples in {num_batches} batches...\")\n",
        "\n",
        "for i in tqdm(range(0, num_examples, BATCH_SIZE)):\n",
        "    # Get batch of examples\n",
        "    batch_end = min(i + BATCH_SIZE, num_examples)\n",
        "    batch_examples = [test_dataset[j] for j in range(i, batch_end)]\n",
        "\n",
        "    # Process batch\n",
        "    batch_predictions, batch_references = process_batch(batch_examples, BATCH_SIZE)\n",
        "\n",
        "    # Add to overall lists\n",
        "    all_predictions.extend(batch_predictions)\n",
        "    all_references.extend(batch_references)\n",
        "\n",
        "# Format for ROUGE - expects a newline after each sentence\n",
        "formatted_predictions = [\"\\n\".join(sent_tokenize(pred.strip())) for pred in all_predictions]\n",
        "formatted_references = [\"\\n\".join(sent_tokenize(ref.strip())) for ref in all_references]\n",
        "\n",
        "# Calculate ROUGE scores\n",
        "rouge_results = rouge.compute(\n",
        "    predictions=formatted_predictions,\n",
        "    references=formatted_references,\n",
        "    use_stemmer=True\n",
        ")\n",
        "\n",
        "# Calculate BLEU score\n",
        "bleu_results = bleu.compute(\n",
        "    predictions=all_predictions,\n",
        "    references=[[ref] for ref in all_references]\n",
        ")\n",
        "\n",
        "# Print results\n",
        "print(\"\\nEvaluation Results on Test Set:\")\n",
        "print(f\"ROUGE-1: {rouge_results['rouge1'] * 100:.2f}%\")\n",
        "print(f\"ROUGE-2: {rouge_results['rouge2'] * 100:.2f}%\")\n",
        "print(f\"ROUGE-L: {rouge_results['rougeL'] * 100:.2f}%\")\n",
        "print(f\"BLEU: {bleu_results['bleu'] * 100:.2f}%\")\n",
        "\n",
        "# Add additional statistics\n",
        "prediction_lens = [len(pred.split()) for pred in all_predictions]\n",
        "reference_lens = [len(ref.split()) for ref in all_references]\n",
        "\n",
        "print(\"\\nAdditional Statistics:\")\n",
        "print(f\"Average prediction length: {np.mean(prediction_lens):.2f} words\")\n",
        "print(f\"Average reference length: {np.mean(reference_lens):.2f} words\")\n",
        "print(f\"Number of test examples: {len(all_predictions)}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "07b34f117e2b44b9b06cb3aa36ddf7e8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "196f89f9b3314d698cbc7abec47c086c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21ff3826331842cca461e4f648256a23": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7084c805624f445ba958c3cea8b2b190",
            "placeholder": "​",
            "style": "IPY_MODEL_bf4379cb253a4849bfec408889802c52",
            "value": " 427/427 [00:00&lt;00:00, 2055.13 examples/s]"
          }
        },
        "367f1323cba945f4848b4757c3814686": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3c1ef3d0307e470a97e05821539990e6": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ac2c42b25c44b54aa306ee0de04d865": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4f46f9ae2b264e16bf9f2bf9ca747100": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "59398cd4d0e5441bbb95e4578e233a14": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_367f1323cba945f4848b4757c3814686",
            "max": 375,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aa37c6bda73048389a33b491645a43a5",
            "value": 375
          }
        },
        "5e62561040314d588c48584120e90354": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "62836dfebe7e4415b4f7b636e79412ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3c1ef3d0307e470a97e05821539990e6",
            "max": 427,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a9ae343b1fa5431981c399034b9df5a8",
            "value": 427
          }
        },
        "7084c805624f445ba958c3cea8b2b190": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74c96253df2446b09fcab538dc9d5273": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "779e4dce210144c5aec52c5fc84de545": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8073bef2ee4e4e8686a4fc1cc95df03c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8f58224d29634d5b8069af78f5d955a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_07b34f117e2b44b9b06cb3aa36ddf7e8",
            "placeholder": "​",
            "style": "IPY_MODEL_b5a9f66e21e94618860345f15254c597",
            "value": "Map: 100%"
          }
        },
        "a9ae343b1fa5431981c399034b9df5a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aa37c6bda73048389a33b491645a43a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "aaa0fae623de4593bc5655b28eba9c7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_74c96253df2446b09fcab538dc9d5273",
            "placeholder": "​",
            "style": "IPY_MODEL_8073bef2ee4e4e8686a4fc1cc95df03c",
            "value": "Map: 100%"
          }
        },
        "abe7c0fb5b2f4befbe55f04726250389": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_aaa0fae623de4593bc5655b28eba9c7c",
              "IPY_MODEL_62836dfebe7e4415b4f7b636e79412ce",
              "IPY_MODEL_21ff3826331842cca461e4f648256a23"
            ],
            "layout": "IPY_MODEL_b41da0516290471f8495dd9210cfcc69"
          }
        },
        "b41da0516290471f8495dd9210cfcc69": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5a9f66e21e94618860345f15254c597": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bcb78d5846f242a681a978e913ebdffd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cb4b6104cd0e4b7b9f91d37562983c5b",
            "max": 1700,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d0dd3e1576404539828538a0fde58b9e",
            "value": 1700
          }
        },
        "be2b4ae4d52b4a728e633f8e1594f71a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4f46f9ae2b264e16bf9f2bf9ca747100",
            "placeholder": "​",
            "style": "IPY_MODEL_196f89f9b3314d698cbc7abec47c086c",
            "value": " 1700/1700 [00:00&lt;00:00, 2121.78 examples/s]"
          }
        },
        "bf4379cb253a4849bfec408889802c52": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c27657a006fa48e8ba8b495fc94504cc": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4419bf3847d4124b6288550993b522f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e19ae6f525544696955b7a138353a93d",
              "IPY_MODEL_bcb78d5846f242a681a978e913ebdffd",
              "IPY_MODEL_be2b4ae4d52b4a728e633f8e1594f71a"
            ],
            "layout": "IPY_MODEL_4ac2c42b25c44b54aa306ee0de04d865"
          }
        },
        "c9192f0e65d14445bfb0b7674acc48ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e62561040314d588c48584120e90354",
            "placeholder": "​",
            "style": "IPY_MODEL_d11c529dd3294723b6779172d4f17c69",
            "value": " 375/375 [00:00&lt;00:00, 2100.23 examples/s]"
          }
        },
        "cb4b6104cd0e4b7b9f91d37562983c5b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d0dd3e1576404539828538a0fde58b9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d11c529dd3294723b6779172d4f17c69": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e19ae6f525544696955b7a138353a93d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c27657a006fa48e8ba8b495fc94504cc",
            "placeholder": "​",
            "style": "IPY_MODEL_779e4dce210144c5aec52c5fc84de545",
            "value": "Map: 100%"
          }
        },
        "e27f10ce9d604fc6988b745cb24d1ec6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f58224d29634d5b8069af78f5d955a9",
              "IPY_MODEL_59398cd4d0e5441bbb95e4578e233a14",
              "IPY_MODEL_c9192f0e65d14445bfb0b7674acc48ce"
            ],
            "layout": "IPY_MODEL_fa37e68a76ff4a9e8d7ddc21bd5fa3ef"
          }
        },
        "fa37e68a76ff4a9e8d7ddc21bd5fa3ef": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
